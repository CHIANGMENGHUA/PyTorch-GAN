


!nvidia-smi


import torch
print("CUDA Available:", torch.cuda.is_available())
print("CUDA Version:", torch.version.cuda)
print("GPU Count:", torch.cuda.device_count())
print("GPU Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")





import pdb
import matplotlib.pyplot as plt

from torch.utils.data import DataLoader
from torch import nn
from torchvision import transforms
from torchvision.datasets import MNIST
from torchvision.utils import make_grid
from tqdm.auto import tqdm





def show(tensor, ch=1, size=(28,28), num=16):
    # tensor.shape = (128, 784)
    data = tensor.detach().cpu().view(-1, ch, *size) # 128 x 1 x 28 x 28
    grid = make_grid(data[:num], nrow=4).permute(1, 2, 0) # 1 x 28 x 28
    plt.imshow(grid)
    plt.show()





epochs = 500
cur_step = 0
info_step = 300
mean_gen_loss = 0
mean_disc_loss = 0
z_dim = 64
lr = 0.00001
loss_func = nn.BCEWithLogitsLoss()
bs = 128
device = "cuda"
data_loader = DataLoader(MNIST(".", 
                               download=True, 
                               transform=transforms.ToTensor()),
                         shuffle=True,
                         batch_size=bs)

# Number of steps = 60000 / 128 = 468.75





class Generator(nn.Module):
    def __init__(self, z_dim, h_dim=128, i_dim=784):
        super().__init__()
        self.gen = nn.Sequential(
            genBlock(z_dim, h_dim), # 64, 128
            genBlock(h_dim, h_dim*2), # 128, 256
            genBlock(h_dim*2, h_dim*4), # 256, 512
            genBlock(h_dim*4, h_dim*8), # 512, 1024
            nn.Linear(h_dim*8, i_dim), # 1024, 784
            nn.Sigmoid()
        )

    def forward(self, noise):
        return self.gen(noise)

def genBlock(input, output):
    return nn.Sequential(
        nn.Linear(input, output),
        nn.BatchNorm1d(output),
        nn.ReLU(inplace=True)
    )

def genNoise(num, z_dim):
    return torch.randn(num, z_dim).to(device)





class Discriminator(nn.Module):
    def __init__(self, i_dim=784, h_dim=256):
        super().__init__()
        self.disc = nn.Sequential(
            discBlock(i_dim, h_dim*4), # 784, 1024
            discBlock(h_dim*4, h_dim*2), # 1024, 512
            discBlock(h_dim*2, h_dim), # 512, 256
            nn.Linear(h_dim, 1) # 256, 1
        )

    def forward(self, img):
        return self.disc(img)

def discBlock(input, output):
    return nn.Sequential(
        nn.Linear(input, output),
        nn.LeakyReLU(0.2)
    )





gen = Generator(z_dim).to(device)
gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)
disc = Discriminator().to(device)
disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)


gen


disc


x, y = next(iter(data_loader))
noise = genNoise(bs, z_dim)
fake = gen(noise)
show(fake)


print(f"x shape: {x.shape} \ny shape: {y.shape}")
print("y[:10] = ", y[:10])





def calc_gen_loss(loss_func, gen, disc, num, z_dim):
    noise = genNoise(num, z_dim)
    fake = gen(noise)
    pred = disc(fake)
    targets = torch.ones_like(pred)
    gen_loss = loss_func(pred, targets)

    return gen_loss

def clac_disc_loss(loss_func, gen, disc, num, real, z_dim):
    noise = genNoise(num, z_dim)
    fake = gen(noise)
    
    disc_fake = disc(fake.detach())
    disc_fake_targets = torch.zeros_like(disc_fake)
    disc_fake_loss = loss_func(disc_fake, disc_fake_targets)
    
    disc_real = disc(real)
    disc_real_targets = torch.ones_like(disc_real)
    disc_real_loss = loss_func(disc_real, disc_real_targets)
    
    disc_loss = (disc_fake_loss + disc_real_loss) / 2
    return disc_loss





for epoch in range(epochs):
    for real, _ in tqdm(data_loader):
        # Discriminator
        disc_opt.zero_grad()
        cur_bs = len(real) # real: 128 x 1 x 28 x 28
        real = real.view(cur_bs, -1) # 128 x 784
        real = real.to(device)
        disc_loss = clac_disc_loss(loss_func, gen, disc, cur_bs, real, z_dim)
        disc_loss.backward(retain_graph=True)
        disc_opt.step()

        # Generator
        gen_opt.zero_grad()
        gen_loss = calc_gen_loss(loss_func, gen, disc, cur_bs, z_dim)
        gen_loss.backward(retain_graph=True)
        gen_opt.step()

        # Visualization & Stats
        mean_disc_loss += disc_loss.item() / info_step
        mean_gen_loss += gen_loss.item() / info_step

        if cur_step % info_step == 0 and cur_step > 0:
            fake_noise = genNoise(cur_bs, z_dim)
            fake = gen(fake_noise)
            show(fake)
            show(real)
            print(f"epoch: {epoch}, step: {cur_step}, gen loss: {mean_gen_loss}, disc loss: {mean_disc_loss}.")
            mean_disc_loss = 0
            mean_gen_loss = 0

        cur_step += 1



